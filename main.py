from fastapi import FastAPI, HTTPException, Header
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import base64
import math

# -----------------------------
# CONFIG
# -----------------------------
API_KEY = "my_secret_key_123"

app = FastAPI(
    title="AI Voice Authenticity Detection API",
    description="Detect whether a voice sample is AI-generated or Human-generated",
    version="1.0"
)

# -----------------------------
# DATA MODELS
# -----------------------------
class VoiceRequest(BaseModel):
    audio_base64: str
    language: str

class VoiceResponse(BaseModel):
    classification: str
    confidence: float
    explanation: str

# -----------------------------
# HOME PAGE
# -----------------------------
@app.get("/", response_class=HTMLResponse)
def home():
    with open("index.html", "r", encoding="utf-8") as f:
        return f.read()

# -----------------------------
# HELPER FUNCTION
# -----------------------------
def shannon_entropy(data: bytes) -> float:
    freq = {}
    for b in data:
        freq[b] = freq.get(b, 0) + 1

    entropy = 0.0
    length = len(data)
    for count in freq.values():
        p = count / length
        entropy -= p * math.log2(p)

    return entropy

# -----------------------------
# DETECTION ENDPOINT
# -----------------------------
@app.post("/detect", response_model=VoiceResponse)
def detect_voice(
    request: VoiceRequest,
    authorization: str = Header(None)
):
    # ðŸ”’ API LOCK
    if authorization != f"Bearer {API_KEY}":
        raise HTTPException(status_code=401, detail="Unauthorized")

    # Base64 decode
    try:
        audio_bytes = base64.b64decode(request.audio_base64, validate=True)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid Base64 input")

    if len(audio_bytes) < 5000:
        raise HTTPException(status_code=400, detail="Audio too short")

    # Analysis
    size_kb = len(audio_bytes) / 1024
    entropy = shannon_entropy(audio_bytes)

    # Decision logic (UNCHANGED)
    if size_kb < 80 and entropy > 7.8:
        classification = "AI-generated"
        confidence = 0.90
    elif entropy < 7.2:
        classification = "Human-generated"
        confidence = 0.85
    else:
        classification = "AI-generated"
        confidence = 0.78

    # Language-based explanation
    language_explanations = {
        "english": {
            "AI-generated": "The English speech shows uniform entropy and synthetic smoothness, common in AI-generated voices.",
            "Human-generated": "The English speech contains natural pauses and acoustic irregularities typical of humans."
        },
        "tamil": {
            "AI-generated": "The Tamil voice exhibits consistent phoneme structure and reduced variation, indicating AI synthesis.",
            "Human-generated": "The Tamil speech shows natural pronunciation variation and expressive entropy."
        },
        "hindi": {
            "AI-generated": "The Hindi audio demonstrates uniform waveform patterns and controlled pitch transitions.",
            "Human-generated": "The Hindi speech includes organic pitch shifts and timing inconsistencies."
        },
        "telugu": {
            "AI-generated": "The Telugu voice shows consistent articulation and limited entropy fluctuation.",
            "Human-generated": "The Telugu speech displays natural stress patterns and entropy diversity."
        },
        "malayalam": {
            "AI-generated": "The Malayalam voice maintains smooth transitions and controlled entropy.",
            "Human-generated": "The Malayalam speech presents natural tonal variations and expressive entropy."
        }
    }

    lang = request.language.lower()
    explanation = language_explanations.get(
        lang, language_explanations["english"]
    )[classification]

    return {
        "classification": classification,
        "confidence": confidence,
        "explanation": explanation
    }
